{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper Classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_whisper.utils import decode_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07757568 -0.07736206 -0.07260132 ... -0.00976562 -0.0178833\n",
      " -0.009552  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/anaconda3/envs/whisper_bench/lib/python3.9/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I appreciate that. Yeah!\n",
      "Time taken for 3 sec : 2.9791951179504395\n",
      " Yeah, you want to see my supervisor? Huh? Yeah, you want to see my supervisor? Fine! I'll be right back!\n",
      "Time taken for 6 sec : 4.602953910827637\n",
      " Yes, but my wallet was stolen. I don't have anything. I don't have any credit cards. I don't have my ID. Don't you have things on file here?\n",
      "Time taken for 10 sec : 5.680836200714111\n",
      " So what's up? What's new? Well, Vegas was awesome. Yeah, I heard. And I got married. Shut up. In Vegas? Yeah, in the old town part. Who'd you marry? Jack! Did he propose to you? Yes, it was very romantic. It was at the slot machines. Oh, real fortune slots? Uh huh, he went big and he realized that the only thing that would make it better was me as his bride. He turned to you and was like, hey, let's get married. And I said, okay. It's really romantic. Yeah, well, you know, because he's leaving the next day. Yeah. But we're going to have a honeymoon cruise. Does that mean you're going to get citizenship too in England or whatever? Oh, I hadn't even thought about that. Yeah, think about that. He's not going to be a citizen though. Yeah, but he'll have like a long visa. Can you go visit them for a long time? Oh, totally. Yeah. How are you going to do the long distancing? So wait, are you going to move there? I guess I'll have an internet husband. Like you need another one of those? I have an internet boyfriend. I guess I'll have a double the two. Is that cheating? Yeah. Yeah. Is that your phone? No, we're kidding. He's calling me now. He loves you. How much did you win in the slot? I think $750. Really? Penny slots. Penny slots? That's what he plays. Wow. Yeah. I'm a big fan of the Wheel of Fortune quarters. Oh. But it just costs so much. Yeah. We played Wheel of Fortune pennies. It's like a giant thing. But the pennies always get you because then you end up spending like, you know. Fifty bucks. Fifty bucks. You're like, wait, but it's just pennies. But so now we're married. Awesome. We have cat children. Cat babies. We renamed Brenda Lumberjant. She needs a new name each state that we go into. That's Brenda. So we are recording spontaneous four.\n",
      "Time taken for 2min11 sec : 54.233384132385254\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "audio_3 = decode_audio(\"audio/3sec.wav\")\n",
    "audio_6 = decode_audio(\"audio/6sec.wav\")\n",
    "audio_10 = decode_audio(\"audio/10sec.wav\")\n",
    "audio_2min11 = decode_audio(\"audio/2min11.wav\")\n",
    "#Load the medium.en model form whisper\n",
    "model = whisper.load_model(\"medium.en\")\n",
    "#transcribe the 3 sec zudio file \n",
    "print(audio_3)\n",
    "\n",
    "start = time()\n",
    "transcript_3 = model.transcribe(audio_3, language=\"English\")\n",
    "print(transcript_3['text'])\n",
    "print(f\"Time taken for 3 sec : {time() - start}\")\n",
    "\n",
    "start = time()\n",
    "transcript_6 = model.transcribe(audio_6, language=\"English\")\n",
    "print(transcript_6['text'])\n",
    "print(f\"Time taken for 6 sec : {time() - start}\")\n",
    "\n",
    "start = time()\n",
    "transcript_10 = model.transcribe(audio_10, language=\"English\")\n",
    "print(transcript_10['text'])\n",
    "print(f\"Time taken for 10 sec : {time() - start}\")\n",
    "\n",
    "start = time()\n",
    "transcript_2min11 = model.transcribe(audio_2min11, language=\"English\")\n",
    "print(transcript_2min11['text'])\n",
    "print(f\"Time taken for 2min11 sec : {time() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With the custom library - Faster-whisper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Desktop/Project/inference_whisper_benchmark/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)350ce/tokenizer.json:   0%|          | 0.00/2.13M [00:00<?, ?B/s]\n",
      "Downloading (…)350ce/vocabulary.txt: 100%|██████████| 422k/422k [00:00<00:00, 5.29MB/s]\n",
      "\n",
      "Downloading (…)1d2350ce/config.json: 100%|██████████| 2.64k/2.64k [00:00<00:00, 14.3MB/s]\n",
      "Downloading (…)350ce/tokenizer.json: 100%|██████████| 2.13M/2.13M [00:01<00:00, 1.16MB/s]\n",
      "Downloading model.bin: 100%|██████████| 1.53G/1.53G [00:52<00:00, 29.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model path :  /home/hugo/Desktop/Project/inference_whisper_benchmark/fast_whisper/models/medium.en\n",
      "tokenizer path :  /home/hugo/Desktop/Project/inference_whisper_benchmark/fast_whisper/models/medium.en/tokenizer.json\n",
      "Loading the model ...\n",
      "tokenizer path :  /home/hugo/Desktop/Project/inference_whisper_benchmark/fast_whisper/models/medium.en/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "from fast_whisper.fast_whisper import WhisperModel\n",
    "\n",
    "\n",
    "model_size = \"medium.en\"\n",
    "\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8\")\n",
    "\n",
    "model_vad = WhisperModel(model_size, vad_activation=True, device=\"cuda\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_3 = model.transcribe(\"audio/3sec.wav\", beam_size=5)\n",
    "segments_6 = model.transcribe(\"audio/6sec.wav\", beam_size=5)\n",
    "segments_10 = model.transcribe(\"audio/10sec.wav\", beam_size=5)\n",
    "segments_2min11 = model.transcribe(\"audio/2min11.wav\", beam_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with VAD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_3_vad = model_vad.transcribe(\"audio/3sec.wav\", beam_size=5)\n",
    "segments_6_vad = model_vad.transcribe(\"audio/6sec.wav\", beam_size=5)\n",
    "segments_10_vad = model_vad.transcribe(\"audio/10sec.wav\", beam_size=5)\n",
    "segments_2min11_vad = model_vad.transcribe(\"audio/2min11.wav\", beam_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "for segment in segments_3:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "print(\"durée 3 sec: \", time() - start, \"s\")\n",
    "\n",
    "start = time()\n",
    "for segment in segments_6:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "print(\"durée 6 sec: \", time() - start, \"s\")\n",
    "\n",
    "start = time()\n",
    "for segment in segments_10:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "print(\"durée 10 sec: \", time() - start, \"s\")\n",
    "\n",
    "start = time()\n",
    "for segment in segments_2min11:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "print(\"durée 2min11: \", time() - start, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07921522110700607 > 0.6\n",
      "not skipping\n",
      "[0.00s -> 2.10s]  No, I appreciate that yeah\n",
      "durée 3 sec:  4.019869089126587 s\n",
      "0.08820515125989914 > 0.6\n",
      "not skipping\n",
      "[0.00s -> 3.60s]  something yeah you want to see my supervisor huh yeah you want to see my\n",
      "[3.60s -> 6.96s]  supervisor fine I'll be right back\n",
      "durée 6 sec:  5.423288106918335 s\n",
      "0.010537789203226566 > 0.6\n",
      "not skipping\n",
      "[0.00s -> 5.32s]  Yes, but I my wallet was stolen. I don't have anything. I don't have any credit cards\n",
      "[5.32s -> 9.04s]  I don't have I don't have my ID. Don't you have things that on file here?\n",
      "durée 10 sec:  6.141629934310913 s\n",
      "0.4093220829963684 > 0.6\n",
      "not skipping\n",
      "[3.50s -> 5.50s]  So what's up? What's new?\n",
      "[5.50s -> 7.50s]  Well, Vegas was awesome.\n",
      "[7.50s -> 9.50s]  Yeah, I heard.\n",
      "[9.50s -> 11.50s]  And I got married.\n",
      "[11.50s -> 13.50s]  Shut up. In Vegas?\n",
      "[13.50s -> 15.50s]  Yeah, in the old town part.\n",
      "[15.50s -> 17.50s]  Who'd you marry?\n",
      "[17.50s -> 19.50s]  Jack!\n",
      "[19.50s -> 21.50s]  Did he propose to you?\n",
      "[21.50s -> 23.50s]  Um, yes.\n",
      "[23.50s -> 25.50s]  It was very romantic.\n",
      "[25.50s -> 27.50s]  It was at the slot machines.\n",
      "[27.50s -> 29.50s]  Oh, real fortune slots?\n",
      "0.9210904836654663 > 0.6\n",
      "not skipping\n",
      "[29.50s -> 31.50s]  He went big and he realized\n",
      "[31.50s -> 33.50s]  that the only thing that would make it better was\n",
      "[33.50s -> 35.50s]  me as his bride.\n",
      "[35.50s -> 37.50s]  He turned to you and was like,\n",
      "[37.50s -> 39.50s]  Hey, let's get married.\n",
      "[39.50s -> 41.50s]  It's really romantic.\n",
      "[41.50s -> 43.50s]  You know, because he's leaving the next day.\n",
      "[43.50s -> 45.50s]  But we're going to have a honeymoon cruise.\n",
      "[45.50s -> 47.50s]  Does that mean you're going to get citizenship too?\n",
      "[47.50s -> 49.50s]  In England or whatever?\n",
      "[49.50s -> 51.50s]  Oh, I hadn't even thought about that.\n",
      "[51.50s -> 53.50s]  Think about that.\n",
      "[53.50s -> 55.50s]  He's not going to be a citizen though.\n",
      "[55.50s -> 57.50s]  Yeah, but he'll have a long visa.\n",
      "0.39535510540008545 > 0.6\n",
      "not skipping\n",
      "[57.50s -> 59.50s]  Oh, how are you going to do the long distancing?\n",
      "[59.50s -> 61.50s]  So, wait, are you going to move there?\n",
      "[61.50s -> 63.50s]  I guess I'll have an internet husband.\n",
      "[65.50s -> 67.50s]  Like you need another one of those?\n",
      "[67.50s -> 69.50s]  I have an internet boyfriend.\n",
      "[69.50s -> 71.50s]  I guess I'll have to juggle the two.\n",
      "[71.50s -> 75.58s]  Is that cheating?\n",
      "[77.58s -> 79.58s]  He's calling me now.\n",
      "[79.58s -> 81.58s]  He loves you.\n",
      "[81.58s -> 83.58s]  How much did he win at the slot?\n",
      "[83.58s -> 85.58s]  I think $750.\n",
      "[85.58s -> 87.58s]  Really? Penny slots.\n",
      "0.09495212882757187 > 0.6\n",
      "not skipping\n",
      "[87.58s -> 89.58s]  Wow.\n",
      "[89.58s -> 91.58s]  I'm a big fan of the Wheel of Fortune quarters.\n",
      "[91.58s -> 93.58s]  But it just costs so much.\n",
      "[93.58s -> 95.58s]  We played Wheel of Fortune pennies.\n",
      "[95.58s -> 97.58s]  It's like a giant thing.\n",
      "[97.58s -> 99.58s]  But the pennies always get you because then you end up spending like, you know,\n",
      "[99.58s -> 101.58s]  50 bucks.\n",
      "[101.58s -> 103.58s]  You're like, wait, but it's just pennies.\n",
      "[103.58s -> 105.58s]  But so, now we're married.\n",
      "[105.58s -> 107.58s]  Awesome.\n",
      "[107.58s -> 109.58s]  We have cat children.\n",
      "[109.58s -> 111.58s]  Cat babies.\n",
      "[111.58s -> 113.58s]  We renamed Brenda, Lumber Janet.\n",
      "[113.58s -> 115.58s]  She needs a new name\n",
      "0.3272857666015625 > 0.6\n",
      "not skipping\n",
      "[115.58s -> 117.58s]  Each state that we go into.\n",
      "[117.58s -> 119.58s]  That's fair enough.\n",
      "[127.58s -> 129.58s]  So we are recording spontaneous four.\n",
      "durée 2min11:  57.22318482398987 s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "for segment in segments_3_vad:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "print(\"durée 3 sec: \", time() - start, \"s\")\n",
    "\n",
    "start = time()\n",
    "for segment in segments_6_vad:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "print(\"durée 6 sec: \", time() - start, \"s\")\n",
    "\n",
    "start = time()\n",
    "for segment in segments_10_vad:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "print(\"durée 10 sec: \", time() - start, \"s\")\n",
    "\n",
    "start = time()\n",
    "for segment in segments_2min11_vad:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "print(\"durée 2min11: \", time() - start, \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "(80, 3000)\n",
      "30.0\n",
      "0.6\n",
      "0.4422985017299652 > 0.6\n",
      "not skipping\n",
      "[0.00s -> 5.00s]  Excuse me?\n",
      "[5.00s -> 10.00s]  Do you have your forms?\n",
      "[10.00s -> 11.00s]  Yeah.\n",
      "[11.00s -> 12.00s]  Let me see them.\n",
      "[12.00s -> 18.00s]  Is there a problem?\n",
      "[18.00s -> 19.00s]  Who told you to get in this line?\n",
      "[19.00s -> 20.00s]  You did.\n",
      "[20.00s -> 21.00s]  No.\n",
      "[21.00s -> 23.00s]  You were standing at the beginning.\n",
      "[23.00s -> 24.00s]  You directed me.\n",
      "[24.00s -> 25.00s]  Okay, but I didn't tell you to get in this line\n",
      "[25.00s -> 27.00s]  if you're filling out this particular form.\n",
      "3000\n",
      "(80, 3000)\n",
      "30.0\n",
      "0.6\n",
      "0.5684831142425537 > 0.6\n",
      "not skipping\n",
      "[27.00s -> 29.00s]  Well, what's the problem?\n",
      "[29.00s -> 30.00s]  This form is a ZX4.\n",
      "[30.00s -> 31.00s]  Let me change it.\n",
      "[31.00s -> 33.00s]  You can't...\n",
      "[33.00s -> 35.00s]  This is not the line for the ZX4.\n",
      "[35.00s -> 37.00s]  If you're going to fill out the ZX4,\n",
      "[37.00s -> 39.00s]  you need to have a different form of ID.\n",
      "[39.00s -> 40.00s]  I'm getting an ID.\n",
      "[40.00s -> 41.00s]  This is why I'm here.\n",
      "[41.00s -> 46.00s]  No, I need another set of ID to prove that this is actually you.\n",
      "[46.00s -> 49.00s]  How am I supposed to get an ID without an ID?\n",
      "[49.00s -> 52.00s]  How does a person get an ID in the first place?\n",
      "[52.00s -> 55.00s]  I don't know, but I need an ID to pass this form along.\n",
      "[55.00s -> 56.00s]  I can't just send it along without an ID.\n",
      "3000\n",
      "(80, 3000)\n",
      "30.0\n",
      "0.6\n",
      "0.017165062949061394 > 0.6\n",
      "not skipping\n",
      "[56.00s -> 58.00s]  I'm here to get an ID.\n",
      "[58.00s -> 60.00s]  No, I need another ID.\n",
      "[60.00s -> 62.00s]  A separate one.\n",
      "[62.00s -> 64.00s]  Like what?\n",
      "[64.00s -> 65.00s]  Like a birth certificate?\n",
      "[65.00s -> 66.00s]  A birth certificate.\n",
      "[66.00s -> 67.00s]  A passport.\n",
      "[67.00s -> 69.00s]  Who the hell has a birth certificate?\n",
      "[69.00s -> 70.00s]  A student ID.\n",
      "[70.00s -> 71.00s]  Didn't you go to school?\n",
      "[71.00s -> 72.00s]  Anything?\n",
      "[72.00s -> 74.00s]  Yes, but my wallet was stolen.\n",
      "[74.00s -> 76.00s]  I don't have anything.\n",
      "[76.00s -> 77.00s]  I don't have any credit cards.\n",
      "[77.00s -> 79.00s]  I don't have my ID.\n",
      "[79.00s -> 81.00s]  Don't you have things on file here?\n",
      "[81.00s -> 82.00s]  Yeah, we keep it on file,\n",
      "[82.00s -> 85.00s]  but we need an ID to access that file.\n",
      "3000\n",
      "(80, 3000)\n",
      "30.0\n",
      "0.6\n",
      "0.01764846034348011 > 0.6\n",
      "not skipping\n",
      "[85.00s -> 87.00s]  That's out of control.\n",
      "[87.00s -> 90.00s]  I don't understand why this is so complicated for people\n",
      "[90.00s -> 91.00s]  when they get here.\n",
      "[91.00s -> 92.00s]  It's just a simple form.\n",
      "[92.00s -> 93.00s]  I just need an ID.\n",
      "[93.00s -> 95.00s]  How long have you been working here?\n",
      "[95.00s -> 98.00s]  I'd say too long.\n",
      "[98.00s -> 99.00s]  Clearly.\n",
      "[99.00s -> 102.00s]  You know, do you have like a supervisor or something?\n",
      "[102.00s -> 103.00s]  Yeah.\n",
      "[103.00s -> 104.00s]  You want to see my supervisor?\n",
      "[104.00s -> 105.00s]  Huh?\n",
      "[105.00s -> 106.00s]  Yeah, you want to see my supervisor?\n",
      "[106.00s -> 113.00s]  Fine, I'll be right back.\n",
      "[113.00s -> 114.00s]  Is that the end?\n",
      "3000\n",
      "(80, 3000)\n",
      "19.62\n",
      "0.6\n",
      "0.10718022286891937 > 0.6\n",
      "not skipping\n",
      "[115.00s -> 117.00s]  He walks away.\n",
      "[120.00s -> 122.00s]  Okay, hold the camera, please.\n",
      "[122.00s -> 127.00s]  Microphones.\n",
      "[127.00s -> 133.00s]  So this is spontaneous 2.\n",
      "durée :  199.70673608779907 s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "print(\"durée : \", time() - start, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2508130669593811 > 0.6\n",
      "[6.58s -> 8.58s]  Excuse me.\n",
      "[8.74s -> 20.07s]  Do you have your forms? Yeah. Let me see them. Is there a problem? Who told you to get in this line? You did.\n",
      "[22.11s -> 27.21s]  You were standing at the beginning, you directed me. Okay, but I didn't tell you to get in this line if you're filling out this particular form.\n",
      "[27.69s -> 35.33s]  Well, what's the problem? What's the problem? Let me change it. This is not the line for the ZX4.\n",
      "[35.33s -> 38.73s]  If you're gonna fill out the ZX4, you need to have a different form of ID.\n",
      "0.009213976562023163 > 0.6\n",
      "[39.05s -> 46.61s]  I'm getting an ID. This is why I'm here. No, I need another set of ID to prove that this is actually you.\n",
      "[46.61s -> 48.71s]  How am I supposed to get an ID without an ID?\n",
      "[49.33s -> 51.37s]  How does the person get an ID in the first place?\n",
      "[51.37s -> 55.23s]  I don't know, but I need an ID to pass this form along.\n",
      "[55.23s -> 57.85s]  I can't just send it along without an ID. I'm here to get an ID.\n",
      "[58.41s -> 63.07s]  No, I need another ID. A separate one. Like what?\n",
      "0.0071402015164494514 > 0.6\n",
      "[64.07s -> 69.07s]  Like a birth certificate? A birth certificate. A password. Who the hell has a birth certificate?\n",
      "[69.77s -> 71.77s]  A student ID. Didn't you go to school?\n",
      "[71.95s -> 77.77s]  Anything? Yes, but my wallet was stolen. I don't have anything. I don't have any credit cards.\n",
      "[77.77s -> 81.49s]  I don't have my ID. Don't you have things on file here?\n",
      "[81.91s -> 86.91s]  Yeah, we keep it on file, but we need an ID to access that file. That's out of control.\n",
      "[87.99s -> 92.65s]  I don't understand why this is so complicated for people when they get here. It's just a simple form.\n",
      "0.004317301791161299 > 0.6\n",
      "[92.73s -> 95.21s]  I just need an ID. How long have you been working here?\n",
      "[95.21s -> 98.58s]  I'd say too long.\n",
      "[98.58s -> 99.78s]  Clearly.\n",
      "[99.78s -> 104.14s]  You know, do you have like a supervisor or something? Yeah, you want to see my supervisor?\n",
      "[104.14s -> 106.86s]  Huh? Yeah, you want to see my supervisor? Fine, I'll be right back.\n",
      "[116.23s -> 127.86s]  He walks away.\n",
      "durée :  147.13541460037231 s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "for segment in segments_vad:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "print(\"durée : \", time() - start, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time : \n",
    "Long file -> \n",
    "Small file -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Whisper.cpp Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pywhispercpp\n",
      "  Obtaining dependency information for pywhispercpp from https://files.pythonhosted.org/packages/43/4b/5b776a79d557392d1fb5e15ffa9af32cc214d3b396482c4b8d686727228b/pywhispercpp-1.1.3-cp39-cp39-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading pywhispercpp-1.1.3-cp39-cp39-macosx_10_9_universal2.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in /Users/hugo/anaconda3/envs/whisper_bench/lib/python3.9/site-packages (from pywhispercpp) (1.26.0)\n",
      "Collecting pydub (from pywhispercpp)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: requests in /Users/hugo/anaconda3/envs/whisper_bench/lib/python3.9/site-packages (from pywhispercpp) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/hugo/anaconda3/envs/whisper_bench/lib/python3.9/site-packages (from pywhispercpp) (4.66.1)\n",
      "Requirement already satisfied: platformdirs in /Users/hugo/anaconda3/envs/whisper_bench/lib/python3.9/site-packages (from pywhispercpp) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hugo/anaconda3/envs/whisper_bench/lib/python3.9/site-packages (from requests->pywhispercpp) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hugo/anaconda3/envs/whisper_bench/lib/python3.9/site-packages (from requests->pywhispercpp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hugo/anaconda3/envs/whisper_bench/lib/python3.9/site-packages (from requests->pywhispercpp) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hugo/anaconda3/envs/whisper_bench/lib/python3.9/site-packages (from requests->pywhispercpp) (2023.7.22)\n",
      "Downloading pywhispercpp-1.1.3-cp39-cp39-macosx_10_9_universal2.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydub, pywhispercpp\n",
      "Successfully installed pydub-0.25.1 pywhispercpp-1.1.3\n"
     ]
    }
   ],
   "source": [
    "! pip install pywhispercpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-03 15:57:46,136] {utils.py:38} INFO - No download directory was provided, models will be downloaded to /Users/hugo/Library/Application Support/pywhispercpp/models\n",
      "[2023-10-03 15:57:46,137] {utils.py:46} INFO - Model medium.en already exists in /Users/hugo/Library/Application Support/pywhispercpp/models\n",
      "[2023-10-03 15:57:46,138] {model.py:221} INFO - Initializing the model ...\n",
      "[2023-10-03 15:57:47,001] {model.py:130} INFO - Transcribing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_no_state: loading model from '/Users/hugo/Library/Application Support/pywhispercpp/models/ggml-medium.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 1024\n",
      "whisper_model_load: n_audio_head  = 16\n",
      "whisper_model_load: n_audio_layer = 24\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 1024\n",
      "whisper_model_load: n_text_head   = 16\n",
      "whisper_model_load: n_text_layer  = 24\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: type          = 4\n",
      "whisper_model_load: mem required  = 1899.00 MB (+   43.00 MB per decoder)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: model ctx     = 1462.35 MB\n",
      "whisper_model_load: model size    = 1462.12 MB\n",
      "whisper_init_state: kv self size  =   42.00 MB\n",
      "whisper_init_state: kv cross size =  140.62 MB\n",
      "whisper_full_with_state: progress =   5%\n",
      "whisper_full_with_state: progress =  10%\n",
      "whisper_full_with_state: progress =  15%\n",
      "whisper_full_with_state: progress =  20%\n",
      "whisper_full_with_state: progress =  25%\n",
      "whisper_full_with_state: progress =  30%\n",
      "whisper_full_with_state: progress =  35%\n",
      "whisper_full_with_state: progress =  40%\n",
      "whisper_full_with_state: progress =  45%\n",
      "whisper_full_with_state: progress =  50%\n",
      "whisper_full_with_state: progress =  55%\n",
      "whisper_full_with_state: progress =  60%\n",
      "whisper_full_with_state: progress =  65%\n",
      "whisper_full_with_state: progress =  70%\n",
      "whisper_full_with_state: progress =  75%\n",
      "whisper_full_with_state: progress =  80%\n",
      "whisper_full_with_state: progress =  85%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-03 15:57:54,455] {model.py:133} INFO - Inference time: 7.454 s\n",
      "No, I'd appreciate that, yeah.\n",
      "durée 3 sec:  7.4557740688323975 s\n",
      "[2023-10-03 15:57:54,457] {model.py:130} INFO - Transcribing ...\n",
      "[2023-10-03 15:58:01,440] {model.py:133} INFO - Inference time: 6.983 s\n",
      "Yeah, you wanna see my supervisor?\n",
      "Huh? Yeah, you wanna see my supervisor? Fine, I'll be right back!\n",
      "durée 6 sec:  6.985399007797241 s\n",
      "[2023-10-03 15:58:01,443] {model.py:130} INFO - Transcribing ...\n",
      "[2023-10-03 15:58:08,735] {model.py:133} INFO - Inference time: 7.292 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: progress =   5%\n",
      "whisper_full_with_state: progress =  10%\n",
      "whisper_full_with_state: progress =  15%\n",
      "whisper_full_with_state: progress =  20%\n",
      "whisper_full_with_state: progress =  25%\n",
      "whisper_full_with_state: progress =  30%\n",
      "whisper_full_with_state: progress =  35%\n",
      "whisper_full_with_state: progress =  40%\n",
      "whisper_full_with_state: progress =  45%\n",
      "whisper_full_with_state: progress =  50%\n",
      "whisper_full_with_state: progress =  55%\n",
      "whisper_full_with_state: progress =  60%\n",
      "whisper_full_with_state: progress =  65%\n",
      "whisper_full_with_state: progress =  70%\n",
      "whisper_full_with_state: progress =  75%\n",
      "whisper_full_with_state: progress =  80%\n",
      "whisper_full_with_state: progress =  85%\n",
      "whisper_full_with_state: progress =  90%\n",
      "whisper_full_with_state: progress =  95%\n",
      "whisper_full_with_state: progress =   5%\n",
      "whisper_full_with_state: progress =  10%\n",
      "whisper_full_with_state: progress =  15%\n",
      "whisper_full_with_state: progress =  20%\n",
      "whisper_full_with_state: progress =  25%\n",
      "whisper_full_with_state: progress =  30%\n",
      "whisper_full_with_state: progress =  35%\n",
      "whisper_full_with_state: progress =  40%\n",
      "whisper_full_with_state: progress =  45%\n",
      "whisper_full_with_state: progress =  50%\n",
      "whisper_full_with_state: progress =  55%\n",
      "whisper_full_with_state: progress =  60%\n",
      "whisper_full_with_state: progress =  65%\n",
      "whisper_full_with_state: progress =  70%\n",
      "whisper_full_with_state: progress =  75%\n",
      "whisper_full_with_state: progress =  80%\n",
      "whisper_full_with_state: progress =  85%\n",
      "whisper_full_with_state: progress =  90%\n",
      "whisper_full_with_state: progress =  95%\n",
      "whisper_full_with_state: progress = 100%\n",
      "whisper_full_with_state: progress = 105%\n",
      "whisper_full_with_state: progress = 110%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, but my wallet was stolen. I don't have anything. I don't have any credit cards. I don't have my ID.\n",
      "Don't you have things on file here?\n",
      "durée 10 sec:  7.294188737869263 s\n",
      "[2023-10-03 15:58:08,747] {model.py:130} INFO - Transcribing ...\n",
      "[2023-10-03 15:58:37,385] {model.py:133} INFO - Inference time: 28.638 s\n",
      "So what's up? What's new?\n",
      "Well, Vegas was awesome.\n",
      "Yeah, I heard.\n",
      "And I got married.\n",
      "Shut up. In Vegas?\n",
      "Yeah, in the old town part.\n",
      "Who'd you marry?\n",
      "Jack!\n",
      "Did he propose to you?\n",
      "Yes, it was very romantic.\n",
      "It was at the slot machines.\n",
      "Oh, real fortune slot?\n",
      "He went big and he realized that the only thing that would make it better was me as his bride.\n",
      "He turned to you and was like...\n",
      "Hey, let's get married.\n",
      "That's really romantic.\n",
      "Yeah, well, you know, cause he's leaving the next day.\n",
      "But we're gonna have a honeymoon cruise.\n",
      "Does that mean we're gonna get citizenship too in England or whatever?\n",
      "Oh, I hadn't even thought about that.\n",
      "Yeah, think about that.\n",
      "I'm not gonna be a citizen though.\n",
      "Yeah, but I'll have a long visa.\n",
      "Can you go visit them for a long time?\n",
      "Oh, totally.\n",
      "How are you gonna get a long distance?\n",
      "So are you gonna move there?\n",
      "I guess I'll have an internet husband.\n",
      "You think you need another one of those?\n",
      "I have an internet boyfriend.\n",
      "I guess I'll have a doublet too.\n",
      "Is that true?\n",
      "Yeah.\n",
      "Yeah.\n",
      "You're calling me now.\n",
      "He loves you.\n",
      "How much did you win at the slot?\n",
      "I think $750.\n",
      "Really?\n",
      "Penny slots.\n",
      "Penny slots?\n",
      "Yeah.\n",
      "I'm a big fan of the real fortune quarters.\n",
      "Oh.\n",
      "But it just costs so much.\n",
      "Yeah.\n",
      "We played real fortune pennies.\n",
      "Yeah.\n",
      "We played real fortune pennies.\n",
      "Yeah.\n",
      "Yeah.\n",
      "We played real fortune pennies.\n",
      "Yeah.\n",
      "We played real fortune pennies.\n",
      "Yeah.\n",
      "We played real fortune pennies.\n",
      "Yeah.\n",
      "durée 2min11:  28.650678873062134 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_full_with_state: progress =   5%\n",
      "whisper_full_with_state: progress =  10%\n",
      "whisper_full_with_state: progress =  15%\n",
      "whisper_full_with_state: progress =  20%\n",
      "whisper_full_with_state: progress =  25%\n",
      "whisper_full_with_state: progress =  30%\n",
      "whisper_full_with_state: progress =  35%\n",
      "whisper_full_with_state: progress =  40%\n",
      "whisper_full_with_state: progress =  45%\n",
      "whisper_full_with_state: progress =  50%\n",
      "whisper_full_with_state: progress =  55%\n",
      "whisper_full_with_state: progress =  60%\n",
      "whisper_full_with_state: progress =  65%\n",
      "whisper_full_with_state: progress =  70%\n",
      "whisper_full_with_state: progress =  75%\n",
      "whisper_full_with_state: progress =  80%\n",
      "whisper_full_with_state: progress =  85%\n",
      "whisper_full_with_state: progress =  90%\n",
      "whisper_full_with_state: progress =  95%\n"
     ]
    }
   ],
   "source": [
    "from pywhispercpp.model import Model\n",
    "\n",
    "model = Model('medium.en', n_threads=6)\n",
    "\n",
    "start = time()\n",
    "segments = model.transcribe('audio/3sec.wav', speed_up=True)\n",
    "for segment in segments:\n",
    "    print(segment.text)\n",
    "print(\"durée 3 sec: \", time() - start, \"s\")\n",
    "\n",
    "start = time()\n",
    "segments = model.transcribe('audio/6sec.wav', speed_up=True)\n",
    "for segment in segments:\n",
    "    print(segment.text)\n",
    "print(\"durée 6 sec: \", time() - start, \"s\")\n",
    "\n",
    "start = time()\n",
    "segments = model.transcribe('audio/10sec.wav', speed_up=True)\n",
    "for segment in segments:\n",
    "    print(segment.text)\n",
    "print(\"durée 10 sec: \", time() - start, \"s\")\n",
    "\n",
    "start = time()\n",
    "segments = model.transcribe('audio/2min11.wav', speed_up=True)\n",
    "for segment in segments:\n",
    "    print(segment.text)\n",
    "print(\"durée 2min11: \", time() - start, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Whisper Jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [git](https://github.com/sanchit-gandhi/whisper-jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper_jax import FlaxWhisperPipline\n",
    "\n",
    "# instantiate pipeline\n",
    "pipeline = FlaxWhisperPipline(\"openai/whisper-large-v2\")\n",
    "\n",
    "# JIT compile the forward call - slow, but we only do once\n",
    "text = pipeline(\"audio.mp3\")\n",
    "\n",
    "# used cached function thereafter - super fast!!\n",
    "text = pipeline(\"audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper_jax import FlaxWhisperPipline\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# instantiate pipeline in bfloat16\n",
    "pipeline = FlaxWhisperPipline(\"openai/whisper-large-v2\", dtype=jnp.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time : \n",
    "Long file -> \n",
    "Small file -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
